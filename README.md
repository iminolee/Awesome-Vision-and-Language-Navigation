# Awesome-Vision-and-Language-Navigation

`Updated on 2024.07.04`

This repo tracks the research papers and codes related to **Vision-and-Language Navigation (VLN)**. The repository will be continuously updated to keep up with the advancements in VLN. Feel free to follow and star!

## Table of Contents
- [Introduction](#introduction)
- [Datasets and Simulators](#datasets-and-simulators)
- [Papers and Codes](#papers-and-codes)
   - [2024](#2024)
   - [2023](#2023)
- [Pre-trained Models](#pre-trained-models)
- [Tools and Libraries](#tools-and-libraries)
- [Acknowledgements](#acknowledgements)
- [Contact](#contact)

## Introduction
This repository aims to provide a comprehensive and up-to-date list of resources for Vision-and-Language (VLN). VLN is an emerging field that integrates computer vision, natural language processing, and robotics to enable agents to navigate environments using linguistic instructions.

## Datasets and Simulators
In VLN tasks, the utilized datasets provide visual assets and scenes, and simulators render these visual assets and provide an environment for the VLN agent. This section will introduce some VLN datasets and simulators commonly used in VLN research.

### Datasets
- **[Matterport3D dataset] Learning from RGB-D Data in Indoor Environments** <br>
   *Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie√üner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang* <br>
   3DV, 2017. [[Paper]](https://arxiv.org/pdf/1709.06158) [[GitHub]](https://github.com/niessner/Matterport)

- **[R2R dataset] Vision-and-Language Navigation: Interpreting visually-grounded
navigation instructions in real environments** <br>
   *Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, Anton van den Hengel* <br>
   CVPR, 2018. [[Paper]](https://arxiv.org/pdf/1711.07280) [[GitHub]](https://github.com/peteanderson80/Matterport3DSimulator/tree/master/tasks/R2R)

- **[RxR dataset] Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding** <br>
   *Alexander Ku, Peter Anderson, Roma Patel, Eugene le, Jason Baldridge* <br>
   EMNLP, 2020. [[Paper]](https://arxiv.org/pdf/2010.07954) [[GitHub]](https://github.com/google-research-datasets/RxR)

### Simulators

## Papers and Codes
### 2024
- **NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models** <br>
   *Gengze Zhou, Yicong Hong, Qi Wu* <br>
   AAAI, 2024. [[Paper]](https://arxiv.org/pdf/2305.16986) [[GitHub]](https://github.com/GengzeZhou/NavGPT)

### 2023

## Pre-trained Models

## Tools and Libraries
A list of tools and libraries that can aid in VLN research and development.

## Acknowledgements
I would like to thank all the researchers and developers who have contributed to the field of Vision-and-Language Navigation.

## Contact
If you have any suggestions for this repository, please create an issue or email **mino@inha.edu**.
